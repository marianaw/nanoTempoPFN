{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIFT-Eval: JAX TimeSeriesForecaster Evaluation\n",
    "\n",
    "This notebook evaluates the JAX TimeSeriesForecaster on GIFT-eval benchmark datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Tensorflow library not found, tensorflow.io.gfile operations will use native shim calls. GCS paths (i.e. 'gs://...') cannot be accessed.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/mariana/Documents/projects/ts/gift-eval/src')\n",
    "sys.path.append('/Users/mariana/Documents/projects/ts/nanoTempoPFN')\n",
    "\n",
    "import os\n",
    "os.environ['GIFT_EVAL'] = '/Users/mariana/Documents/projects/ts/gift-eval-data'\n",
    "\n",
    "# from gift_eval.data import Dataset\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import pyarrow.compute as pc\n",
    "import datasets\n",
    "import math \n",
    "\n",
    "from src.tsf import TimeSeriesForecaster\n",
    "from src.config import load_config, cfg_to_training_config\n",
    "from src.data.time_features import compute_batch_time_features\n",
    "from src.data.frequency import parse_frequency\n",
    "from gluonts.itertools import Map\n",
    "from gluonts.time_feature import get_seasonality, norm_freq_str\n",
    "from gluonts.dataset import DataEntry\n",
    "from gluonts.model.forecast import QuantileForecast\n",
    "from gluonts.model.predictor import Predictor\n",
    "from gluonts.model.evaluation import evaluate_model\n",
    "from gluonts.ev.metrics import (  # using the same metrics as the tempoPFN evaluation\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    MASE,\n",
    "    MSE,\n",
    "    MSIS,\n",
    "    ND,\n",
    "    NRMSE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "from gluonts.transform import Transformation\n",
    "from gluonts.dataset.split import TestData, TrainingDataset, split\n",
    "from gluonts.dataset.common import ProcessDataEntry\n",
    "from tqdm import tqdm\n",
    "from enum import Enum\n",
    "from typing import Iterator\n",
    "from pathlib import Path\n",
    "from collections.abc import Iterable\n",
    "from functools import cached_property\n",
    "from toolz import compose\n",
    "from pandas.tseries.frequencies import to_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "import warnings\n",
    "from linear_operator.utils.cholesky import NumericalWarning\n",
    "\n",
    "# --- Setup Logging as in tempoPFN ---\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logging.getLogger(\"matplotlib\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"matplotlib.font_manager\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"PIL\").setLevel(logging.WARNING)\n",
    "logger = logging.getLogger(\"gift_eval_runner\")\n",
    "\n",
    "\n",
    "# Filter out specific gluonts warnings\n",
    "class WarningFilter(logging.Filter):\n",
    "    def __init__(self, text_to_filter: str) -> None:\n",
    "        super().__init__()\n",
    "        self.text_to_filter = text_to_filter\n",
    "\n",
    "    def filter(self, record: logging.LogRecord) -> bool:\n",
    "        return self.text_to_filter not in record.getMessage()\n",
    "\n",
    "\n",
    "gts_logger = logging.getLogger(\"gluonts.model.forecast\")\n",
    "gts_logger.addFilter(WarningFilter(\"The mean prediction is not stored in the forecast data\"))\n",
    "\n",
    "# Filter out numerical warnings\n",
    "warnings.filterwarnings(\"ignore\", category=NumericalWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load a GIFT-eval dataset with train/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load dataset - this gives you BOTH train and test\n",
    "# dataset_name = 'm4_monthly'  # Change to any GIFT-eval dataset\n",
    "# dataset = Dataset(dataset_name)\n",
    "\n",
    "# print(f\"Dataset: {dataset_name}\")\n",
    "# print(f\"Frequency: {dataset.freq}\")\n",
    "# print(f\"Prediction length: {dataset.prediction_length}\")\n",
    "# print(f\"Number of test series: {len(dataset.test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we borrow this from TempoPFN's repo.\n",
    "# Please check their submission notebook at https://github.com/automl/TempoPFN/blob/main/examples/gift_eval/gift_eval_submission.ipynb \n",
    "\n",
    "TEST_SPLIT = 0.1\n",
    "MAX_WINDOW = 20\n",
    "\n",
    "M4_PRED_LENGTH_MAP = {\n",
    "    \"A\": 6,\n",
    "    \"Q\": 8,\n",
    "    \"M\": 18,\n",
    "    \"W\": 13,\n",
    "    \"D\": 14,\n",
    "    \"H\": 48,\n",
    "    \"h\": 48,\n",
    "    \"Y\": 6,\n",
    "}\n",
    "\n",
    "PRED_LENGTH_MAP = {\n",
    "    \"M\": 12,\n",
    "    \"W\": 8,\n",
    "    \"D\": 30,\n",
    "    \"H\": 48,\n",
    "    \"h\": 48,\n",
    "    \"T\": 48,\n",
    "    \"S\": 60,\n",
    "    \"s\": 60,\n",
    "    \"min\": 48,\n",
    "}\n",
    "\n",
    "TFB_PRED_LENGTH_MAP = {\n",
    "    \"A\": 6,\n",
    "    \"Y\": 6,\n",
    "    \"H\": 48,\n",
    "    \"h\": 48,\n",
    "    \"Q\": 8,\n",
    "    \"D\": 14,\n",
    "    \"M\": 18,\n",
    "    \"W\": 13,\n",
    "    \"U\": 8,\n",
    "    \"T\": 8,\n",
    "    \"min\": 8,\n",
    "    \"us\": 8,\n",
    "}\n",
    "\n",
    "\n",
    "class Term(Enum):\n",
    "    SHORT = \"short\"\n",
    "    MEDIUM = \"medium\"\n",
    "    LONG = \"long\"\n",
    "\n",
    "    @property\n",
    "    def multiplier(self) -> int:\n",
    "        if self == Term.SHORT:\n",
    "            return 1\n",
    "        elif self == Term.MEDIUM:\n",
    "            return 10\n",
    "        elif self == Term.LONG:\n",
    "            return 15\n",
    "\n",
    "\n",
    "def itemize_start(data_entry: DataEntry) -> DataEntry:\n",
    "    data_entry[\"start\"] = data_entry[\"start\"].item()\n",
    "    return data_entry\n",
    "\n",
    "\n",
    "class MultivariateToUnivariate(Transformation):\n",
    "    def __init__(self, field):\n",
    "        self.field = field\n",
    "\n",
    "    def __call__(self, data_it: Iterable[DataEntry], is_train: bool = False) -> Iterator:\n",
    "        for data_entry in data_it:\n",
    "            item_id = data_entry[\"item_id\"]\n",
    "            val_ls = list(data_entry[self.field])\n",
    "            for id, val in enumerate(val_ls):\n",
    "                univariate_entry = data_entry.copy()\n",
    "                univariate_entry[self.field] = val\n",
    "                univariate_entry[\"item_id\"] = item_id + \"_dim\" + str(id)\n",
    "                yield univariate_entry\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        term: Term | str = Term.SHORT,\n",
    "        to_univariate: bool = False,\n",
    "        storage_path: str = None,\n",
    "        max_windows: int | None = None,\n",
    "    ):\n",
    "        storage_path = Path(storage_path)\n",
    "        self.hf_dataset = datasets.load_from_disk(str(storage_path / name)).with_format(\"numpy\")\n",
    "        process = ProcessDataEntry(\n",
    "            self.freq,\n",
    "            one_dim_target=self.target_dim == 1,\n",
    "        )\n",
    "\n",
    "        self.gluonts_dataset = Map(compose(process, itemize_start), self.hf_dataset)\n",
    "        if to_univariate:\n",
    "            self.gluonts_dataset = MultivariateToUnivariate(\"target\").apply(self.gluonts_dataset)\n",
    "\n",
    "        self.term = Term(term)\n",
    "        self.name = name\n",
    "        self.max_windows = max_windows if max_windows is not None else MAX_WINDOW\n",
    "\n",
    "    @cached_property\n",
    "    def prediction_length(self) -> int:\n",
    "        freq = norm_freq_str(to_offset(self.freq).name)\n",
    "        if freq.endswith(\"E\"):\n",
    "            freq = freq[:-1]\n",
    "        pred_len = M4_PRED_LENGTH_MAP[freq] if \"m4\" in self.name else PRED_LENGTH_MAP[freq]\n",
    "        return self.term.multiplier * pred_len\n",
    "\n",
    "    @cached_property\n",
    "    def freq(self) -> str:\n",
    "        return self.hf_dataset[0][\"freq\"]\n",
    "\n",
    "    @cached_property\n",
    "    def target_dim(self) -> int:\n",
    "        return target.shape[0] if len((target := self.hf_dataset[0][\"target\"]).shape) > 1 else 1\n",
    "\n",
    "    @cached_property\n",
    "    def past_feat_dynamic_real_dim(self) -> int:\n",
    "        if \"past_feat_dynamic_real\" not in self.hf_dataset[0]:\n",
    "            return 0\n",
    "        elif len((past_feat_dynamic_real := self.hf_dataset[0][\"past_feat_dynamic_real\"]).shape) > 1:\n",
    "            return past_feat_dynamic_real.shape[0]\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    @cached_property\n",
    "    def windows(self) -> int:\n",
    "        if \"m4\" in self.name:\n",
    "            return 1\n",
    "        w = math.ceil(TEST_SPLIT * self._min_series_length / self.prediction_length)\n",
    "        return min(max(1, w), self.max_windows)\n",
    "\n",
    "    @cached_property\n",
    "    def _min_series_length(self) -> int:\n",
    "        if self.hf_dataset[0][\"target\"].ndim > 1:\n",
    "            lengths = pc.list_value_length(pc.list_flatten(pc.list_slice(self.hf_dataset.data.column(\"target\"), 0, 1)))\n",
    "        else:\n",
    "            lengths = pc.list_value_length(self.hf_dataset.data.column(\"target\"))\n",
    "        return min(lengths.to_numpy())\n",
    "\n",
    "    @cached_property\n",
    "    def sum_series_length(self) -> int:\n",
    "        if self.hf_dataset[0][\"target\"].ndim > 1:\n",
    "            lengths = pc.list_value_length(pc.list_flatten(self.hf_dataset.data.column(\"target\")))\n",
    "        else:\n",
    "            lengths = pc.list_value_length(self.hf_dataset.data.column(\"target\"))\n",
    "        return sum(lengths.to_numpy())\n",
    "\n",
    "    @property\n",
    "    def training_dataset(self) -> TrainingDataset:\n",
    "        training_dataset, _ = split(self.gluonts_dataset, offset=-self.prediction_length * (self.windows + 1))\n",
    "        return training_dataset\n",
    "\n",
    "    @property\n",
    "    def validation_dataset(self) -> TrainingDataset:\n",
    "        validation_dataset, _ = split(self.gluonts_dataset, offset=-self.prediction_length * self.windows)\n",
    "        return validation_dataset\n",
    "\n",
    "    @property\n",
    "    def test_data(self) -> TestData:\n",
    "        _, test_template = split(self.gluonts_dataset, offset=-self.prediction_length * self.windows)\n",
    "        test_data = test_template.generate_instances(\n",
    "            prediction_length=self.prediction_length,\n",
    "            windows=self.windows,\n",
    "            distance=self.prediction_length,\n",
    "        )\n",
    "        return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'electricity/H'\n",
    "dataset = Dataset(name=dataset_name, storage_path='/Users/mariana/Documents/projects/ts/gift-eval-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load your trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load config\n",
    "cfg = load_config('../conf/training.yaml')\n",
    "training_config = cfg_to_training_config(cfg)\n",
    "\n",
    "# Create forecaster\n",
    "forecaster = TimeSeriesForecaster(config=training_config, seed=42)\n",
    "\n",
    "# Load checkpoint if you have one\n",
    "# forecaster.load('/path/to/checkpoint')\n",
    "\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.windows * dataset.prediction_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create predictor wrapper (using the proven eval.py implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.model.forecast import Quantile\n",
    "from sympy import Q\n",
    "from src.data.containers import NpBatchTSContainer\n",
    "\n",
    "\n",
    "class TempoPredictorWrapper(Predictor):\n",
    "    \"\"\"Wrapper to make TimeSeriesForecaster compatible with GluonTS Predictor interface.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, forecaster: TimeSeriesForecaster,\n",
    "                 prediction_length: int,\n",
    "                 context_length: int,\n",
    "                 time_dim: int = 6,\n",
    "                 batch_size: int = 64):\n",
    "        self.forecaster = forecaster\n",
    "        self.prediction_length = prediction_length\n",
    "        self.context_length = context_length\n",
    "        self.time_dim = time_dim\n",
    "        self.quantiles = forecaster.quantiles\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def _convert_to_npbatchc(self, items: list[dict]) -> NpBatchTSContainer:\n",
    "        batch_size = len(items)\n",
    "        history_values_list = []\n",
    "        start_dates = []\n",
    "        frequencies = []\n",
    "\n",
    "        for entry in items:\n",
    "            target = entry[\"target\"]\n",
    "\n",
    "            if target.ndim == 1:\n",
    "                # target is (seq_len,), then (seq_len, dim) where dim =1\n",
    "                target = target.reshape(-1, 1)\n",
    "            else:\n",
    "                # (n_channels, seq_len) -> (seq_len, n_channels)\n",
    "                target = target.T\n",
    "\n",
    "            if target.ndim == 2:\n",
    "                # we add a \"dim\" dimension at the end because our model expects that.\n",
    "                target = np.expand_dims(target, -1)\n",
    "            # target at this point is of shape (seq_len, n_channels, dim)\n",
    "\n",
    "            if self.context_length is not None and len(target) > self.context_length:\n",
    "                target = target[-self.context_length:]\n",
    "\n",
    "            history_values_list.append(target)\n",
    "            start_dates.append(entry[\"start\"].to_timestamp().to_datetime64())\n",
    "            frequencies.append(parse_frequency(entry[\"freq\"]))\n",
    "\n",
    "        # debug_msg = [str(h.shape[0]) for h in history_values_list[:4]]\n",
    "        # min_shape = min([h.shape[0] for h in history_values_list])\n",
    "        # print(f'\\n\\n =-================= \\n {', '.join(debug_msg)} and min h_len is {min_shape} \\n -=-------------------- \\n\\n')\n",
    "\n",
    "        # (batch_size, seq_len, n_channels, dim)\n",
    "        history_values = np.stack(history_values_list, axis=0)\n",
    "        n_channels = history_values.shape[2]\n",
    "\n",
    "        future_values = np.zeros(\n",
    "            (batch_size, self.prediction_length, n_channels),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        history_tf, future_tf = compute_batch_time_features(\n",
    "            start=start_dates,\n",
    "            history_length=self.context_length,\n",
    "            future_length=self.prediction_length,\n",
    "            batch_size=batch_size,\n",
    "            frequency=frequencies,\n",
    "            K_max=self.time_dim,\n",
    "            include_extra=True,\n",
    "        )\n",
    "\n",
    "        return NpBatchTSContainer(\n",
    "            history=history_values,\n",
    "            future=future_values,\n",
    "            start=start_dates,\n",
    "            frequency=frequencies,\n",
    "            history_time_features=history_tf,\n",
    "            future_time_features=future_tf\n",
    "        )\n",
    "\n",
    "    def _handle_missing_data(self, x):\n",
    "        return x\n",
    "\n",
    "    def _predict_batch(self, batch: NpBatchTSContainer) -> np.ndarray:\n",
    "        x = jnp.array(batch.history)\n",
    "        t_hist = jnp.array(batch.history_time_features)\n",
    "        t_future = jnp.array(batch.future_time_features)\n",
    "        x = self._handle_missing_data(x)\n",
    "        x_scaled, (m, iqr) = self.forecaster.robust_scaler.scale(x)\n",
    "        preds = self.forecaster.model.apply(\n",
    "            self.forecaster.model_state.params, x_scaled, t_hist, t_future, training=False)\n",
    "        preds = self.forecaster.robust_scaler.inverse_scale(preds, medians=m, iqrs=iqr)\n",
    "        return np.asarray(preds)\n",
    "\n",
    "    def _to_quantile_forecast(self, preds, h_len, test_batch) -> list[QuantileForecast]:\n",
    "\n",
    "        forecasts = []\n",
    "\n",
    "        # pred is of shape (batch_size, f_len, n_channels, n_quantiles)\n",
    "        for i in range(len(test_batch)):\n",
    "            pred = np.array(preds[i])  # (f_len, n_channels, n_quantiles)\n",
    "            pred = pred.transpose(2, 0, 1)  # (n_quantiles, f_len, n_channels)\n",
    "            if pred.shape[-1] == 1:\n",
    "                pred = pred.squeeze(-1)  # if n_channels =1 then we make pred (n_quantiles, f_len)\n",
    "\n",
    "            test_item = test_batch[i]\n",
    "            start = test_item['start']\n",
    "            item_id = test_item['item_id']\n",
    "            forecast_start = start + h_len\n",
    "            forecast = QuantileForecast(\n",
    "                forecast_arrays=pred,\n",
    "                start_date=forecast_start,\n",
    "                forecast_keys=[str(q.item())\n",
    "                               for q in self.quantiles],\n",
    "                item_id=item_id\n",
    "            )\n",
    "            forecasts.append(forecast)\n",
    "        return forecasts\n",
    "\n",
    "    def predict(self, dataset) -> Iterator[QuantileForecast]:\n",
    "        \"\"\"Generate forecasts for the test data.\"\"\"\n",
    "\n",
    "        def get_h_len(item):\n",
    "            target = item['target']\n",
    "            if target.ndim == 1:\n",
    "                # target is (seq_len,), then (seq_len, dim) where dim =1\n",
    "                seq_len = target.shape[0]\n",
    "            else:\n",
    "                # (n_channels, seq_len) -> (seq_len, n_channels)\n",
    "                seq_len = target.shape[1]\n",
    "            h_len = min(seq_len, self.context_length)\n",
    "            return h_len\n",
    "\n",
    "        dataset_list = list(dataset)\n",
    "\n",
    "        # Process in batches and yield forecasts\n",
    "        for i in range(0, len(dataset_list), self.batch_size):\n",
    "            items = dataset_list[i: i+self.batch_size]\n",
    "            batch = self._convert_to_npbatchc(items)\n",
    "            preds = self._predict_batch(batch)\n",
    "            h_len = get_h_len(items[0])\n",
    "            batch_forecasts = self._to_quantile_forecast(preds, h_len, items)\n",
    "            \n",
    "            # Yield each forecast individually\n",
    "            for forecast in batch_forecasts:\n",
    "                yield forecast\n",
    "\n",
    "\n",
    "# TODO: have a min_seq_len for the dataset and set the context length to that value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create predictor and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor created!\n"
     ]
    }
   ],
   "source": [
    "# Create predictor\n",
    "predictor = TempoPredictorWrapper(\n",
    "    forecaster=forecaster,\n",
    "    prediction_length=dataset.prediction_length,\n",
    "    context_length=min(dataset.prediction_length + 200, 2048),  # Your model's max context\n",
    "    time_dim=training_config.time_dim\n",
    ")\n",
    "\n",
    "print(\"Predictor created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate using GluonTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7400it [33:56,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Results ===\n",
      "         MSE[mean]      MSE[0.5]     MAE[0.5]  MASE[0.5]  MAPE[0.5]  sMAPE[0.5]       MSIS    RMSE[mean]  NRMSE[mean]   ND[0.5]  mean_weighted_sum_quantile_loss\n",
      "None  1.217201e+08  1.217201e+08  1651.986552   8.289392   1.636119    0.604522  342.69398  11032.682977     5.208214  0.779856                         0.584604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from gluonts.time_feature import get_seasonality\n",
    "\n",
    "# Get seasonality for MASE computation\n",
    "seasonality = get_seasonality(dataset.freq)\n",
    "\n",
    "METRICS = (\n",
    "    MSE(forecast_type=\"mean\"),\n",
    "    MSE(forecast_type=0.5),\n",
    "    MAE(),\n",
    "    MASE(),\n",
    "    MAPE(),\n",
    "    SMAPE(),\n",
    "    MSIS(),\n",
    "    RMSE(),\n",
    "    NRMSE(),\n",
    "    ND(),\n",
    "    MeanWeightedSumQuantileLoss(quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "print(\"Evaluating...\")\n",
    "metrics_df = evaluate_model(\n",
    "    model=predictor,\n",
    "    test_data=dataset.test_data,\n",
    "    metrics=METRICS,\n",
    "    seasonality=seasonality,\n",
    "    axis=None\n",
    ")\n",
    "\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(metrics_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to results_jax_forecaster_electricity_H.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Convert metrics to dict\n",
    "results = {\n",
    "    'dataset': dataset_name,\n",
    "    'model': 'JAX-TimeSeriesForecaster',\n",
    "    'metrics': metrics_df.iloc[0].to_dict()\n",
    "}\n",
    "\n",
    "output_file = f'results_jax_forecaster_electricity_H.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MSE[mean]': 121720093.67227477,\n",
       " 'MSE[0.5]': 121720093.67227477,\n",
       " 'MAE[0.5]': 1651.986551590653,\n",
       " 'MASE[0.5]': 8.289392081848204,\n",
       " 'MAPE[0.5]': 1.6361188371878346,\n",
       " 'sMAPE[0.5]': 0.6045219486683339,\n",
       " 'MSIS': 342.6939797861846,\n",
       " 'RMSE[mean]': 11032.682977058426,\n",
       " 'NRMSE[mean]': 5.20821429576941,\n",
       " 'ND[0.5]': 0.7798556336934874,\n",
       " 'mean_weighted_sum_quantile_loss': 0.5846041410210856}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df.iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'electricity_H'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name.replace('/', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanotsjaxinf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
