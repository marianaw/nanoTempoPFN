{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimalistic GIFT-Eval Evaluation\n",
    "\n",
    "Simple windowed evaluation following TempoPFN approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from src.config import load_config, cfg_to_training_config\n",
    "from src.tsf import TimeSeriesForecaster\n",
    "from src.data.time_features import compute_batch_time_features\n",
    "from src.data.frequency import parse_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset and Extract Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency: 1H\n",
      "Prediction length: 24\n"
     ]
    }
   ],
   "source": [
    "from gluonts.dataset.repository import get_dataset\n",
    "\n",
    "dataset = get_dataset(\"electricity\")\n",
    "test_data = dataset.test\n",
    "\n",
    "# Get metadata\n",
    "freq = dataset.metadata.freq  # Get frequency from metadata\n",
    "prediction_length = dataset.metadata.prediction_length\n",
    "\n",
    "print(f\"Frequency: {freq}\")\n",
    "print(f\"Prediction length: {prediction_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context length: 512\n",
      "Quantiles: [0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9]\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"../conf/training.yaml\")\n",
    "training_config = cfg_to_training_config(cfg)\n",
    "\n",
    "forecaster = TimeSeriesForecaster(config=training_config)\n",
    "# forecaster.load(\"../checkpoints/model.pkl\")  # Uncomment when you have a checkpoint\n",
    "\n",
    "context_length = 512\n",
    "quantiles = forecaster.quantiles\n",
    "\n",
    "print(f\"Context length: {context_length}\")\n",
    "print(f\"Quantiles: {quantiles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Windowed Test Dataset\n",
    "\n",
    "Split each time series into windows: [context | target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariana/.Envs/xlstm/lib/python3.12/site-packages/gluonts/dataset/common.py:263: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  return pd.Period(val, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 44940 windows\n"
     ]
    }
   ],
   "source": [
    "class TestDataset:\n",
    "    \"\"\"Split time series into non-overlapping windows.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, context_length, prediction_length, max_windows=20):\n",
    "        self.data = list(data)\n",
    "        self.context_length = context_length\n",
    "        self.prediction_length = prediction_length\n",
    "        self.max_windows = max_windows\n",
    "        self.window_size = context_length + prediction_length\n",
    "    \n",
    "    def create_windows(self):\n",
    "        \"\"\"Generate (context, target, metadata) tuples.\"\"\"\n",
    "        windows = []\n",
    "        \n",
    "        for ts in self.data:\n",
    "            target = np.array(ts['target'])\n",
    "            \n",
    "            # Calculate how many windows we can fit\n",
    "            max_possible = (len(target) - self.window_size) // self.prediction_length + 1\n",
    "            num_windows = min(max_possible, self.max_windows)\n",
    "            \n",
    "            # Create windows\n",
    "            for i in range(num_windows):\n",
    "                end_idx = len(target) - i * self.prediction_length\n",
    "                start_idx = end_idx - self.window_size\n",
    "                \n",
    "                if start_idx < 0:\n",
    "                    break\n",
    "                \n",
    "                context = target[start_idx:start_idx + self.context_length]\n",
    "                pred_target = target[start_idx + self.context_length:end_idx]\n",
    "                \n",
    "                windows.append({\n",
    "                    'context': context,\n",
    "                    'target': pred_target,\n",
    "                    'start': ts['start'],\n",
    "                    'item_id': ts.get('item_id', '')\n",
    "                })\n",
    "        \n",
    "        return windows\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = TestDataset(\n",
    "    data=test_data,\n",
    "    context_length=context_length,\n",
    "    prediction_length=prediction_length,\n",
    "    max_windows=20\n",
    ")\n",
    "\n",
    "windows = test_dataset.create_windows()\n",
    "print(f\"Created {len(windows)} windows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Predictions on All Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5 predictions\n",
      "Prediction shape: (24, 9)  # (pred_len, num_quantiles)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariana/Documents/projects/ts/nanoTempoPFN/src/data/frequency.py:285: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  offset = pd.tseries.frequencies.to_offset(freq_str)\n"
     ]
    }
   ],
   "source": [
    "# Parse frequency once\n",
    "frequency = parse_frequency(freq)\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "for window in windows[:5]:  # Start with first 5 for testing\n",
    "    # Prepare context\n",
    "    context = jnp.array(window['context'][None, ..., None, None])  # (1, context_len, 1, 1)\n",
    "    \n",
    "    # Compute time features\n",
    "    history_tf, future_tf = compute_batch_time_features(\n",
    "        start=[np.datetime64(window['start'])],\n",
    "        history_length=context_length,\n",
    "        future_length=prediction_length,\n",
    "        batch_size=1,\n",
    "        frequency=[frequency],\n",
    "        K_max=training_config.time_dim,\n",
    "        include_extra=False,\n",
    "    )\n",
    "    \n",
    "    # Predict (uncomment when you have a trained model)\n",
    "    # preds = forecaster.predict(context, history_tf, future_tf)\n",
    "    # preds = np.array(preds[0, :, 0, :])  # (pred_len, num_quantiles)\n",
    "    \n",
    "    # For now, use random predictions for testing\n",
    "    preds = np.random.randn(prediction_length, len(quantiles))\n",
    "    \n",
    "    predictions.append(preds)\n",
    "    targets.append(window['target'])\n",
    "\n",
    "print(f\"Generated {len(predictions)} predictions\")\n",
    "print(f\"Prediction shape: {predictions[0].shape}  # (pred_len, num_quantiles)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (5, 24, 9)\n",
      "Targets shape: (5, 24)\n",
      "\n",
      "=== Metrics ===\n",
      "MSE (median): 191.6480\n",
      "MSE (mean): 188.5279\n"
     ]
    }
   ],
   "source": [
    "from gluonts.ev.metrics import MeanWeightedSumQuantileLoss, MSE\n",
    "\n",
    "# Stack predictions and targets\n",
    "all_preds = np.stack(predictions)  # (num_windows, pred_len, num_quantiles)\n",
    "all_targets = np.stack(targets)    # (num_windows, pred_len)\n",
    "\n",
    "print(f\"Predictions shape: {all_preds.shape}\")\n",
    "print(f\"Targets shape: {all_targets.shape}\")\n",
    "\n",
    "# Initialize metrics\n",
    "quantile_loss = MeanWeightedSumQuantileLoss(\n",
    "    quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    ")\n",
    "mse_mean = MSE(forecast_type=\"mean\")\n",
    "mse_median = MSE(forecast_type=0.5)\n",
    "\n",
    "# Compute metrics\n",
    "# Note: GluonTS metrics expect specific input format\n",
    "# For now, let's compute simple metrics manually\n",
    "\n",
    "# Extract median (0.5 quantile) predictions\n",
    "median_idx = list(quantiles).index(0.5)\n",
    "median_preds = all_preds[:, :, median_idx]\n",
    "\n",
    "# Compute MSE for median\n",
    "mse_value = np.mean((median_preds - all_targets) ** 2)\n",
    "\n",
    "# Compute mean prediction (average across quantiles)\n",
    "mean_preds = np.mean(all_preds, axis=2)\n",
    "mse_mean_value = np.mean((mean_preds - all_targets) ** 2)\n",
    "\n",
    "print(f\"\\n=== Metrics ===\")\n",
    "print(f\"MSE (median): {mse_value:.4f}\")\n",
    "print(f\"MSE (mean): {mse_mean_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compute Quantile Loss\n",
    "\n",
    "Manual implementation of weighted quantile loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Quantile Losses ===\n",
      "Q0.10000000149011612: 0.8942\n",
      "Q0.20000000298023224: 1.8204\n",
      "Q0.30000001192092896: 2.7448\n",
      "Q0.4000000059604645: 3.6181\n",
      "Q0.5: 4.5030\n",
      "Q0.6000000238418579: 5.4280\n",
      "Q0.699999988079071: 6.2855\n",
      "Q0.800000011920929: 7.1625\n",
      "Q0.8999999761581421: 8.1268\n",
      "\n",
      "Average Quantile Loss: 4.5092\n"
     ]
    }
   ],
   "source": [
    "def quantile_loss(y_true, y_pred, quantile):\n",
    "    \"\"\"Compute quantile loss for a single quantile.\"\"\"\n",
    "    error = y_true - y_pred\n",
    "    return np.mean(np.maximum(quantile * error, (quantile - 1) * error))\n",
    "\n",
    "# Compute loss for each quantile\n",
    "quantile_losses = {}\n",
    "for i, q in enumerate(quantiles):\n",
    "    q_preds = all_preds[:, :, i]\n",
    "    loss = quantile_loss(all_targets, q_preds, q)\n",
    "    quantile_losses[q.item()] = loss\n",
    "\n",
    "print(\"\\n=== Quantile Losses ===\")\n",
    "for q, loss in quantile_losses.items():\n",
    "    print(f\"Q{q}: {loss:.4f}\")\n",
    "\n",
    "# Average quantile loss\n",
    "avg_ql = np.mean(list(quantile_losses.values()))\n",
    "print(f\"\\nAverage Quantile Loss: {avg_ql:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Loading dataset and extracting metadata (frequency from `dataset.metadata.freq`)\n",
    "2. Creating windowed test dataset (non-overlapping windows)\n",
    "3. Running predictions on all windows\n",
    "4. Computing metrics: MSE (mean, median) and Quantile Loss\n",
    "\n",
    "Next: Port this to `eval.py` script."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
